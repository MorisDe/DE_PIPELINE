version: '3.8'
services:
  # Kafka broker running in KRaft mode (no Zookeeper needed)
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    hostname: kafka
    container_name: kafka
    user: "0:0"  # Run as root to avoid permission issues
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_NUM_PARTITIONS: 4
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
    volumes:
      - kafka-logs:/tmp/kraft-combined-logs
      

    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server localhost:9092 --list"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Schema Registry for Avro schema management
  schema-registry:
    image: confluentinc/cp-schema-registry:7.6.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8082:8081"  # Fixed: Changed from 8081:8081 to 8082:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/subjects || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Debezium Connect for Change Data Capture from MySQL
  debezium:
    image: debezium/connect:2.7.3.Final
    hostname: debezium
    container_name: debezium
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: debezium-cluster
      CONFIG_STORAGE_TOPIC: debezium_connect_configs
      OFFSET_STORAGE_TOPIC: debezium_connect_offsets
      STATUS_STORAGE_TOPIC: debezium_connect_statuses

      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1

      # Converters
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      # REST and plugin path
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      CONNECT_REST_PORT: 8083
      CONNECT_PLUGIN_PATH: /kafka/connect

      # ✅ Proper log4j configuration via environment variables
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGER_ORG_APACHE_KAFKA_CONNECT_RUNTIME_REST: WARN
      CONNECT_LOG4J_LOGGER_ORG_REFLECTIONS: ERROR

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
  
  jobmanager:
    image: apache/flink:1.18.0-scala_2.12-java11
    platform: linux/amd64
    container_name: jobmanager
    hostname: jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    volumes:
      - ./flink-jars:/opt/flink/lib/extra
    entrypoint: >
      /bin/sh -c "mkdir -p /opt/flink/lib/extra && cp -n /opt/flink/lib/extra/*.jar /opt/flink/lib/ && /docker-entrypoint.sh jobmanager"
    environment:
      - TZ=Asia/Kolkata
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        jobmanager.memory.process.size: 2048m
        # CRITICAL OPTIMIZATIONS FOR LOW LATENCY
        execution.buffer-timeout: 0ms
        execution.checkpointing.mode: AT_LEAST_ONCE
        execution.checkpointing.interval: 500ms
        execution.checkpointing.timeout: 30s
        execution.checkpointing.min-pause: 100ms
        execution.checkpointing.max-concurrent-checkpoints: 1
        # Pipeline optimizations
        pipeline.object-reuse: true
        pipeline.operator-chaining: true
        # Network optimizations
        taskmanager.network.memory.fraction: 0.2
        taskmanager.network.netty.num-arenas: 2
        # State backend optimizations
        state.backend: rocksdb
        state.backend.rocksdb.writebuffer.size: 32m
        state.backend.rocksdb.writebuffer.count: 2
    depends_on:
      postgres:
        condition: service_healthy

  taskmanager:
    image: apache/flink:1.18.0-scala_2.12-java11
    platform: linux/amd64
    container_name: taskmanager
    hostname: taskmanager
    command: taskmanager
    volumes:
      - ./flink-jars:/opt/flink/lib/extra
    entrypoint: >
      /bin/sh -c "mkdir -p /opt/flink/lib/extra && cp -n /opt/flink/lib/extra/*.jar /opt/flink/lib/ && /docker-entrypoint.sh taskmanager"
    environment:
      - TZ=Asia/Kolkata
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 4
        taskmanager.memory.process.size: 2048m
        taskmanager.memory.managed.fraction: 0.4
        # SAME OPTIMIZATIONS AS JOBMANAGER
        execution.buffer-timeout: 0ms
        execution.checkpointing.mode: AT_LEAST_ONCE
        execution.checkpointing.interval: 500ms
        execution.checkpointing.timeout: 30s
        execution.checkpointing.min-pause: 100ms
        execution.checkpointing.max-concurrent-checkpoints: 1
        pipeline.object-reuse: true
        pipeline.operator-chaining: true
        taskmanager.network.memory.fraction: 0.2
        taskmanager.network.netty.num-arenas: 2
        state.backend: rocksdb
        state.backend.rocksdb.writebuffer.size: 32m
        state.backend.rocksdb.writebuffer.count: 2
    depends_on:
      - jobmanager
      - postgres
  

  starrocks-fe-0:
    image: starrocks/fe-ubuntu:3.3-latest
    hostname: starrocks-fe-0
    container_name: starrocks-fe-0
    command:
      - /bin/bash
      - -c
      - |
        # Add optimizations to fe.conf
        echo "stream_load_default_timeout_second = 10" >> /opt/starrocks/fe/conf/fe.conf
        echo "load_parallel_instance_num = 4" >> /opt/starrocks/fe/conf/fe.conf
        echo "max_load_timeout_second = 30" >> /opt/starrocks/fe/conf/fe.conf
        /opt/starrocks/fe_entrypoint.sh starrocks-fe-0
    environment:
      - HOST_TYPE=FQDN
      - TZ=Asia/Kolkata
    ports:
      - "8030:8030"
      - "9020:9020"
      - "9030:9030"
    volumes:
      - singleton_fe0_data:/opt/starrocks/fe/meta
    healthcheck:
      test: ["CMD", "mysql", "-h127.0.0.1", "-P9030", "-uroot", "-e", "SHOW FRONTENDS;"]
      interval: 5s
      timeout: 5s
      retries: 30
    mem_limit: 4g

  starrocks-be-0:
    image: starrocks/be-ubuntu:3.3-latest
    hostname: starrocks-be-0
    container_name: starrocks-be-0
    command:
      - /bin/bash
      - -c
      - |
        # Add optimizations to be.conf
        echo "streaming_load_max_mb = 1024" >> /opt/starrocks/be/conf/be.conf
        echo "load_process_max_memory_limit_bytes = 2147483648" >> /opt/starrocks/be/conf/be.conf
        echo "streaming_load_rpc_max_alive_time_sec = 30" >> /opt/starrocks/be/conf/be.conf
        /opt/starrocks/be_entrypoint.sh starrocks-fe-0
    environment:
      - HOST_TYPE=FQDN
      - TZ=Asia/Kolkata
    ports:
      - "8040:8040"
    depends_on:
      starrocks-fe-0:
          condition: service_healthy
    volumes:
      - singleton_be0_data:/opt/starrocks/be/storage
    mem_limit: 4g



  connect-sink:
    image: debezium/connect:2.7.3.Final
    hostname: connect-sink
    container_name: connect-sink
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8084:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: connect-sink-cluster
      CONFIG_STORAGE_TOPIC: connect_sink_configs
      OFFSET_STORAGE_TOPIC: connect_sink_offsets
      STATUS_STORAGE_TOPIC: connect_sink_statuses

      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1

      # Converters
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      # REST and plugin path
      CONNECT_REST_ADVERTISED_HOST_NAME: connect-sink
      CONNECT_REST_PORT: 8083
      CONNECT_PLUGIN_PATH: /kafka/connect

      # ✅ Proper log4j configuration
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGER_ORG_APACHE_KAFKA_CONNECT_RUNTIME_REST: WARN
      CONNECT_LOG4J_LOGGER_ORG_REFLECTIONS: ERROR

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # Source MySQL database (the spaghetti DB you want to extract from)
  mysql:
    image: mysql:8.1
    hostname: mysql
    container_name: mysql
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: new_db
      MYSQL_USER: debezium
      MYSQL_PASSWORD: debezium
    command:
      - --server-id=1
      - --log-bin=mysql-bin
      - --binlog-format=row
      - --binlog-row-image=full
      - --expire-logs-days=10
      - --gtid-mode=on
      - --enforce-gtid-consistency=on
    volumes:
      - mysql-data:/var/lib/mysql
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping -h localhost -u root -proot"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Target PostgreSQL database (clean, transformed structure)
  postgres:
    image: postgres:15
    hostname: postgres
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: rathishnair
      POSTGRES_PASSWORD: root
      POSTGRES_DB: new_db_master
    command:
      - postgres
      - -c
      - wal_level=logical
      - -c
      - max_wal_senders=10
      - -c
      - max_replication_slots=10
      - -c
      - wal_sender_timeout=60s
      - -c
      - wal_writer_delay=10ms              # Faster WAL writes
      - -c
      - commit_delay=100                   # Micro-batch commits
      - -c
      - checkpoint_completion_target=0.9
      - -c
      - shared_preload_libraries=pg_stat_statements
    volumes:
      - postgres-data:/var/lib/postgresql/data
    # ADD HEALTHCHECK
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rathishnair -d new_db_master"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Kafka management UI (KRaft compatible)
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    hostname: kafdrop
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
      JVM_OPTS: "-Xms128M -Xmx256M"
      SERVER_SERVLET_CONTEXTPATH: "/"
      # Optional: allow topic deletion (risky, but useful for dev)
      KAFKA_PROPERTIES: |
        delete.topic.enable=true
      # Optional: disable UI readonly mode (for interactivity)
      KAFKA_ALLOW_TOPIC_CREATION: "true"
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/"]
      interval: 30s
      timeout: 10s
      retries: 5


volumes:
  kafka-logs:    # Persistent storage for Kafka logs
  mysql-data:    # Persistent storage for MySQL data
  postgres-data:
  singleton_fe0_data:
  singleton_be0_data:
